{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8add309",
   "metadata": {},
   "source": [
    "# SHAP Feature Selection for Yield Prediction\n",
    "This notebook demonstrates how to apply SHAP (SHapley Additive exPlanations) to interpret a machine learning model for crop yield prediction. Each code cell is explained in detail to help you understand what it does, how to use it, and how to adapt it to your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f13ee",
   "metadata": {},
   "source": [
    "# Cell 1: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# No inputs or outputs here, just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Configure Matplotlib for batch job processing\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Start a Dask cluster for parallel processing\n",
    "client = Client()  # On Setonix, let Dask manage resources across CPUs\n",
    "\n",
    "# Load the dataset into a Dask DataFrame\n",
    "file_path = \"/scratch/pawsey0988/mibrahim/updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "data = gpd.read_file(file_path)\n",
    "data = dd.from_pandas(data, npartitions=8)\n",
    "\n",
    "# Define target variable and features\n",
    "target_variable = 'yield'\n",
    "features = data.drop(columns=[target_variable, 'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', \n",
    "                              'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry'])\n",
    "target = data[target_variable]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features = dd.from_dask_array(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "# Handle missing values\n",
    "features = features.fillna(features.median().compute())\n",
    "target = target.fillna(target.median().compute())\n",
    "\n",
    "# Sample for SHAP feature importance calculation\n",
    "sample = features.sample(frac=0.1).compute()  # Smaller subset for SHAP\n",
    "target_sample = target.loc[sample.index].compute()\n",
    "\n",
    "# Train a preliminary model on CPU for SHAP\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "prelim_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "prelim_model.fit(sample, target_sample)\n",
    "\n",
    "# Use SHAP for feature importance\n",
    "explainer = shap.Explainer(prelim_model)\n",
    "shap_values = explainer(sample)\n",
    "\n",
    "# Select important features (Top 50%)\n",
    "shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "important_features = sample.columns[shap_importance > np.percentile(shap_importance, 50)]\n",
    "\n",
    "# Filter important features\n",
    "features = features[important_features]\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.compute(), target.compute(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the final model using Dask with XGBoost on CPU (use ROCm-compatible ML tools if required)\n",
    "model = XGBRegressor(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Save prediction plot\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(f\"XGBoost Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "plt.savefig(\"prediction_plot.png\", bbox_inches='tight')\n",
    "\n",
    "# Feature importance plot\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(important_features)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(important_features)), important_features[indices], rotation=90)\n",
    "plt.savefig(\"feature_importance_plot.png\", bbox_inches='tight')\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094ba24",
   "metadata": {},
   "source": [
    "# Cell 2: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# No inputs or outputs here, just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Configure Matplotlib for batch job processing\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Setup Dask Client across all available work partitions\n",
    "client = Client(n_workers=256, threads_per_worker=1, memory_limit='4GB')\n",
    "print(client)\n",
    "\n",
    "# Load data as Dask DataFrame\n",
    "file_path = \"/scratch/pawsey0988/mibrahim/updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "data = gpd.read_file(file_path)\n",
    "data = dd.from_pandas(data, npartitions=256)\n",
    "\n",
    "# Define target and features\n",
    "target_column = 'yield'\n",
    "drop_columns = ['ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "features = data.drop(columns=[target_column] + drop_columns)\n",
    "target = data[target_column]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features.fillna(0))  # Handling NaNs by filling with 0\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model on full dataset using Dask-optimized XGBoost\n",
    "xgb_model = XGBRegressor(n_estimators=150, n_jobs=256, tree_method='hist')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# SHAP analysis\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Save SHAP values to a file for later combination if needed\n",
    "joblib.dump(shap_values, \"/scratch/pawsey0988/mibrahim/shap_values_partition.pkl\")\n",
    "\n",
    "# Save model to disk\n",
    "joblib.dump(xgb_model, \"/scratch/pawsey0988/mibrahim/final_yield_model.pkl\")\n",
    "\n",
    "# Evaluate and plot results\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test.compute(), y_pred.compute(), alpha=0.5)\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(f\"XGBoost Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "plt.savefig(\"/scratch/pawsey0988/mibrahim/prediction_plot.png\", bbox_inches='tight')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_summary_plot.png\", bbox_inches='tight')\n",
    "\n",
    "# Feature importance plot\n",
    "importances = xgb_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(important_features)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(important_features)), important_features[indices], rotation=90)\n",
    "plt.savefig(\"feature_importance_plot.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "print(\"Distributed SHAP processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877128ef",
   "metadata": {},
   "source": [
    "# Cell 3: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# No inputs or outputs here, just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Configure Matplotlib for batch job processing\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"/scratch/pawsey0988/mibrahim/updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = ['ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=[target_column] + drop_columns)\n",
    "    target = data[target_column]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features.fillna(0))  # Handling NaNs by filling with 0\n",
    "    print(features)\n",
    "\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(features.compute(), target.compute())\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(features.compute())\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "     # Summary plot for SHAP values  important_features = features.columns  # Get the feature names\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, features.compute(), feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "    # # Use SHAP for feature importance\n",
    "    # explainer = shap.Explainer(preliminary_rf_model)\n",
    "    # shap_values = explainer(features)\n",
    "\n",
    "    # # Select important features (Top 50%)\n",
    "    # shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    # important_features = sample.columns[shap_importance > np.percentile(shap_importance, 70)]\n",
    "\n",
    "    # # Filter important features\n",
    "    # selected_features = features[important_features]\n",
    "    # selected_feature_names = selected_features.columns\n",
    "    # print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "    \n",
    "    \n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(shap_importance)[-num_top_features:]\n",
    "    selected_features = features[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Saving the datasets\n",
    "    joblib.dump(X_train, \"/scratch/pawsey0988/mibrahim/X_train_selected.pkl\")\n",
    "    joblib.dump(X_test, \"/scratch/pawsey0988/mibrahim/X_test_selected.pkl\")\n",
    "    joblib.dump(y_train, \"/scratch/pawsey0988/mibrahim/y_train.pkl\")\n",
    "    joblib.dump(y_test, \"/scratch/pawsey0988/mibrahim/y_test.pkl\")\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained Random Forest model\n",
    "    model_path = \"/scratch/pawsey0988/mibrahim/rf_model.pkl\"\n",
    "    joblib.dump(rf_model, model_path)\n",
    "    print(\"Random Forest model trained and saved.\")\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"/scratch/pawsey0988/mibrahim/prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Final SHAP analysis with the trained model\n",
    "    explainer_final = shap.TreeExplainer(rf_model)\n",
    "    shap_values_final = explainer_final.shap_values(X_test.compute())\n",
    "\n",
    "    # Summary plot for SHAP values after training\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_final, X_test.compute(), feature_names=selected_feature_names, show=False)\n",
    "    plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_summary_plot_after_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Force plot for a single prediction (first test instance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.force_plot(explainer_final.expected_value, shap_values_final[0], X_test.compute().iloc[0], matplotlib=True)\n",
    "    plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_force_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaee56",
   "metadata": {},
   "source": [
    "# Cell 4: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# No inputs or outputs here, just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Configure Matplotlib for batch job processing\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development\\Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "    data = data[0:2000,:]\n",
    "    print(data)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = ['ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=[target_column] + drop_columns)\n",
    "    target = data[target_column]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features.fillna(0))  # Handling NaNs by filling with 0\n",
    "    print(features)\n",
    "\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(features.compute(), target.compute())\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(features.compute())\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "     # Summary plot for SHAP values  important_features = features.columns  # Get the feature names\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, features.compute(), feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "    # # Use SHAP for feature importance\n",
    "    # explainer = shap.Explainer(preliminary_rf_model)\n",
    "    # shap_values = explainer(features)\n",
    "\n",
    "    # # Select important features (Top 50%)\n",
    "    # shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    # important_features = sample.columns[shap_importance > np.percentile(shap_importance, 70)]\n",
    "\n",
    "    # # Filter important features\n",
    "    # selected_features = features[important_features]\n",
    "    # selected_feature_names = selected_features.columns\n",
    "    # print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "\n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(shap_importance)[-num_top_features:]\n",
    "    selected_features = features[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Saving the datasets\n",
    "    # joblib.dump(X_train, \"/scratch/pawsey0988/mibrahim/X_train_selected.pkl\")\n",
    "    # joblib.dump(X_test, \"/scratch/pawsey0988/mibrahim/X_test_selected.pkl\")\n",
    "    # joblib.dump(y_train, \"/scratch/pawsey0988/mibrahim/y_train.pkl\")\n",
    "    # joblib.dump(y_test, \"/scratch/pawsey0988/mibrahim/y_test.pkl\")\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained Random Forest model\n",
    "    # model_path = \"/scratch/pawsey0988/mibrahim/rf_model.pkl\"\n",
    "    # joblib.dump(rf_model, model_path)\n",
    "    # print(\"Random Forest model trained and saved.\")\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Final SHAP analysis with the trained model\n",
    "    explainer_final = shap.TreeExplainer(rf_model)\n",
    "    shap_values_final = explainer_final.shap_values(X_test.compute())\n",
    "\n",
    "    # Summary plot for SHAP values after training\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_final, X_test.compute(), feature_names=selected_feature_names, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_after_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Force plot for a single prediction (first test instance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.force_plot(explainer_final.expected_value, shap_values_final[0], X_test.compute().iloc[0], matplotlib=True)\n",
    "    plt.savefig(\"shap_force_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529426e",
   "metadata": {},
   "source": [
    "# Cell 5: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\" ,  just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup Dask Client with adjusted resources\n",
    "client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development\\Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "data = dgpd.read_file(file_path, npartitions=256)\n",
    "\n",
    "target_column = 'yield'\n",
    "drop_columns = [target_column,'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area']\n",
    "features = data.drop(columns=drop_columns)\n",
    "target = data[target_column]\n",
    "print(features)\n",
    "print(target)\n",
    "\n",
    "\n",
    "preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "with ProgressBar():\n",
    "    preliminary_rf_model.fit(features, target.compute())\n",
    "\n",
    "explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "shap_values = explainer.shap_values(features)\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Summary plot for SHAP values  important_features = features.columns  # Get the feature names\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, features.compute(), feature_names=features.columns, show=False)\n",
    "plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "# Select top 30% features based on SHAP importance\n",
    "num_top_features = int(0.3 * features.shape[1])\n",
    "top_feature_indices = np.argsort(shap_importance)[-num_top_features:]\n",
    "selected_features = features[:, top_feature_indices]\n",
    "selected_feature_names = features.columns[top_feature_indices]\n",
    "print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "   \n",
    "rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "with ProgressBar():\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test.compute())\n",
    "mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "r2 = r2_score(y_test.compute(), y_pred)\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "#model_path = \"/scratch/pawsey0988/mibrahim/rf_model.pkl\"\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "# plt.savefig(\"/scratch/pawsey0988/mibrahim/prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "# Final SHAP analysis with the trained model\n",
    "explainer_final = shap.TreeExplainer(rf_model)\n",
    "shap_values_final = explainer_final.shap_values(X_test.compute())\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_final, X_test.compute(), feature_names=selected_feature_names, show=False)\n",
    "plt.savefig(\"shap_summary_plot_after_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "# Force plot for a single prediction (first test instance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.force_plot(explainer_final.expected_value, shap_values_final[0], X_test.compute().iloc[0], matplotlib=True)\n",
    "plt.savefig(\"shap_force_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "print(\"SHAP analysis completed and plots saved.\")\n",
    "print(\"Distributed Random Forest processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d1a50",
   "metadata": {},
   "source": [
    "# Cell 6: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Configure Matplotlib for batch job processing\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development/Desktop/Food Agility project-homework/WA-Rainfall-Zone-Analysis/Complete dataset/updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Select the initial 2000 rows for analysis\n",
    "    data = data.head(2000)  # Use head() to take the first 2000 rows\n",
    "    print(data)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = ['ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=[target_column] + drop_columns)\n",
    "    target = data[target_column]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features.fillna(0))  # Handling NaNs by filling with 0\n",
    "    print(features)\n",
    "\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(features.compute(), target.compute())\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(features.compute())\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, features.compute(), feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(shap_importance)[-num_top_features:]\n",
    "    selected_features = features[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Final SHAP analysis with the trained model\n",
    "    explainer_final = shap.TreeExplainer(rf_model)\n",
    "    shap_values_final = explainer_final.shap_values(X_test.compute())\n",
    "\n",
    "    # Summary plot for SHAP values after training\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_final, X_test.compute(), feature_names=selected_feature_names, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_after_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Force plot for a single prediction (first test instance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.force_plot(explainer_final.expected_value, shap_values_final[0], X_test.compute().iloc[0], matplotlib=True)\n",
    "    plt.savefig(\"shap_force_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ac4ea",
   "metadata": {},
   "source": [
    "# Cell 7: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "    \n",
    "    # Use the initial 2000 rows for analysis\n",
    "    data = data.head(2000)  # Retrieves the first 2000 rows\n",
    "    print(data)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = ['ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=[target_column] + drop_columns)\n",
    "    target = data[target_column]\n",
    "\n",
    "    # Scale features using Dask\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features.fillna(0))  # Handling NaNs by filling with 0\n",
    "\n",
    "    # Persist the scaled features in memory\n",
    "    features_scaled = features_scaled.persist()\n",
    "\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(features_scaled, target.compute())\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(features_scaled)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, features_scaled, feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]  # Get the mean absolute SHAP values\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Final SHAP analysis with the trained model\n",
    "    explainer_final = shap.TreeExplainer(rf_model)\n",
    "    shap_values_final = explainer_final.shap_values(X_test.compute())\n",
    "\n",
    "    # Summary plot for SHAP values after training\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_final, X_test.compute(), feature_names=selected_feature_names, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_after_training_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    # Force plot for a single prediction (first test instance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.force_plot(explainer_final.expected_value, shap_values_final[0], X_test.compute().iloc[0], matplotlib=True)\n",
    "    plt.savefig(\"shap_force_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8c130",
   "metadata": {},
   "source": [
    "# Cell 8: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "    \n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [target_column,'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "    \n",
    "    features = features.fillna(features.median())\n",
    "\n",
    "    # Scale features using Dask\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)  # Handling NaNs by filling with 0\n",
    "    \n",
    "    # Sample for SHAP feature importance calculation\n",
    "    sample = features_scaled.sample(frac=0.05).compute()  # Smaller subset for SHAP\n",
    "    target_sample = target.loc[sample.index].compute()\n",
    "\n",
    "\n",
    "    # Persist the scaled features in memory\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(features_scaled, target_sample)  # No need to call .compute() here\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, features_scaled.compute(), feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    # plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "\n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]  # Get the mean absolute SHAP values\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())  # Call compute on train sets here\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())  # Call compute on test set\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "    # Save the trained Random Forest model\n",
    "    # model_path = \"/scratch/pawsey0988/mibrahim/rf_model.pkl\"\n",
    "    # joblib.dump(rf_model, model_path)\n",
    "    # print(\"Random Forest model trained and saved.\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    # plt.savefig(\"/scratch/pawsey0988/mibrahim/prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d63383",
   "metadata": {},
   "source": [
    "# Cell 9: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.array as da  # Import dask.array\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "    \n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [target_column,'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "    \n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Scale features using Dask\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)  \n",
    "    \n",
    "    # Compute the number of samples\n",
    "    num_samples = int(0.05 * features_scaled.shape[0].compute())\n",
    "\n",
    "    # Get random indices for sampling\n",
    "    sample_indices = np.random.choice(features_scaled.shape[0].compute(), num_samples, replace=False)\n",
    "\n",
    "    # Create a boolean mask as a NumPy array\n",
    "    mask = np.isin(np.arange(features_scaled.shape[0].compute()), sample_indices)\n",
    "\n",
    "    # Apply the mask using .loc to select rows\n",
    "    sample = features_scaled.loc[mask].compute()\n",
    "    target_sample = target.loc[mask].compute()\n",
    "    \n",
    "    print(sample)\n",
    "\n",
    "    \n",
    "    # # Sample for SHAP feature importance calculation\n",
    "    # sample = features_scaled.sample(frac=0.05).compute()  # Smaller subset for SHAP\n",
    "    # target_sample = target.loc[sample.index].compute()\n",
    "\n",
    "\n",
    "    # Persist the scaled features in memory\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(sample, target_sample)  # No need to call .compute() here\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, features_scaled.compute(), feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    # plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "\n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]  # Get the mean absolute SHAP values\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())  # Call compute on train sets here\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())  # Call compute on test set\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "    # Save the trained Random Forest model\n",
    "    # model_path = \"/scratch/pawsey0988/mibrahim/rf_model.pkl\"\n",
    "    # joblib.dump(rf_model, model_path)\n",
    "    # print(\"Random Forest model trained and saved.\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    # plt.savefig(\"/scratch/pawsey0988/mibrahim/prediction_plot_rf.png\", bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d709735",
   "metadata": {},
   "source": [
    "# Cell 10: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.array as da  # Import dask.array\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client with adjusted resources\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame with specified partitions\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "    \n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [target_column,'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "    \n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Scale features using Dask\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features).compute()  # Ensure features_scaled is computed here\n",
    "    target = target.compute()  # Ensure target is computed\n",
    "    \n",
    "    # Compute the number of samples\n",
    "    num_samples = int(0.05 * len(features_scaled))\n",
    "\n",
    "    # Get random indices for sampling\n",
    "    sample_indices = np.random.choice(len(features_scaled), num_samples, replace=False)\n",
    "\n",
    "    # Select rows by index\n",
    "    sample = features_scaled[sample_indices]\n",
    "    target_sample = target.iloc[sample_indices]\n",
    "    \n",
    "    print(sample)\n",
    "\n",
    "    # Preliminary SHAP analysis to select top 30% features\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(sample, target_sample)  # No need to call .compute() here\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, sample, feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "\n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]  # Get the mean absolute SHAP values\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor with selected features\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())  # Call compute on train sets here\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())  # Call compute on test set\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa02259",
   "metadata": {},
   "source": [
    "# Cell 11: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "#inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [target_column, 'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "    \n",
    "    # Fill NaN values\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Scale features and convert to Dask DataFrame\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled_np = scaler.fit_transform(features).compute()  # Compute to NumPy array\n",
    "    features_scaled_df = pd.DataFrame(features_scaled_np, columns=features.columns)  # Convert to Pandas DataFrame\n",
    "    features_scaled_dask_df = dd.from_pandas(features_scaled_df, npartitions=16)  # Convert to Dask DataFrame\n",
    "    \n",
    "    # Compute the number of samples\n",
    "    num_samples = int(0.05 * len(features_scaled_dask_df))\n",
    "    \n",
    "    # Sample indices\n",
    "    sample_indices = np.random.choice(len(features_scaled_dask_df), num_samples, replace=False)\n",
    "    \n",
    "    # Select rows by index using .iloc\n",
    "    sample = features_scaled_dask_df.iloc[sample_indices]\n",
    "    target_sample = target.iloc[sample_indices]\n",
    "    \n",
    "    # Compute the samples\n",
    "    sample = sample.compute()\n",
    "    target_sample = target_sample.compute()\n",
    "    \n",
    "    print(sample)\n",
    "\n",
    "    # Preliminary SHAP analysis\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(sample, target_sample)\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, sample, feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    \n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b6721",
   "metadata": {},
   "source": [
    "# Cell 12: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [target_column, 'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "    \n",
    "    # Fill NaN values\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Scale features and convert to Dask DataFrame\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled_np = scaler.fit_transform(features).compute()  # Compute to NumPy array\n",
    "    features_scaled_df = pd.DataFrame(features_scaled_np, columns=features.columns)  # Convert to Pandas DataFrame\n",
    "    features_scaled_dask_df = dd.from_pandas(features_scaled_df, npartitions=16)  # Convert to Dask DataFrame\n",
    "    \n",
    "    # Sample 5% of the rows randomly\n",
    "    sample = features_scaled_dask_df.sample(frac=0.05, random_state=42)\n",
    "    target_sample = target.sample(frac=0.05, random_state=42)\n",
    "    \n",
    "    # Compute the samples\n",
    "    sample = sample.compute()\n",
    "    target_sample = target_sample.compute()\n",
    "    \n",
    "    print(sample)\n",
    "\n",
    "    # Preliminary SHAP analysis\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(sample, target_sample)\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, sample, feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    \n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233a89f",
   "metadata": {},
   "source": [
    "# Cell 13: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:64081' processes=16 threads=32, memory=119.21 GiB>\n",
      "(<dask_expr.expr.Scalar: expr=FromGraph(afe1101).size() // 56, dtype=int64>, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.Ibrah\\.conda\\envs\\odc1\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 147.61 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        et_morton_actual  et_morton_potential  et_morton_wet  et_tall_crop  \\\n",
      "457            -1.606658             1.397905       0.884088      1.567229   \n",
      "1239            2.315870            -0.213628       0.777707     -0.074257   \n",
      "816            -0.819731            -0.543766      -0.986967     -0.520637   \n",
      "862            -0.300157            -0.917375      -1.177381     -0.831200   \n",
      "441             1.127409            -0.236733       0.234180     -0.171146   \n",
      "...                  ...                  ...            ...           ...   \n",
      "436788         -0.926672             0.350311      -0.011659      0.513132   \n",
      "435862          1.238386            -0.799105      -0.367454     -0.678514   \n",
      "436869          0.796496             0.292351       0.677583      0.007493   \n",
      "437071         -0.128647             0.871563       0.935043      0.816774   \n",
      "437041         -1.626835             1.665776       1.184458      1.655035   \n",
      "\n",
      "        et_short_crop  evap_morton_lake  evap_pan  evap_syn  max_temp  \\\n",
      "457          1.484960          1.146026  1.443488  1.220831  1.384295   \n",
      "1239         0.103456          0.660778  0.257722  0.244922  0.677162   \n",
      "816         -0.624075         -0.916450 -1.496186 -1.097716 -0.993100   \n",
      "862         -0.897904         -1.028860 -1.490922 -1.139855 -1.042380   \n",
      "441         -0.115978          0.039382 -0.541678 -0.480292  0.131563   \n",
      "...               ...               ...       ...       ...       ...   \n",
      "436788       0.457640          0.229990  0.304044  0.447371  0.533455   \n",
      "435862      -0.589460         -0.370460 -0.230410 -0.483345 -0.286762   \n",
      "436869       0.016300          0.005869 -0.219882 -0.263491 -0.149847   \n",
      "437071       0.897744          1.149517  0.896400  1.203426  0.591505   \n",
      "437041       1.612912          1.421115  1.105550  1.538398  1.504745   \n",
      "\n",
      "        min_temp  ...  Soil_SMOS_Summary_min_30m_mean  Soil_Smectite_30m_min  \\\n",
      "457     1.182392  ...                        2.407600               0.155107   \n",
      "1239    0.084644  ...                       -0.316554               0.651470   \n",
      "816    -0.817228  ...                       -0.382442              -0.001431   \n",
      "862    -0.693118  ...                       -0.562652               0.458766   \n",
      "441    -0.575156  ...                       -0.576410              -0.227340   \n",
      "...          ...  ...                             ...                    ...   \n",
      "436788 -0.413271  ...                        0.301369              -2.075459   \n",
      "435862 -1.128752  ...                       -0.399839               1.356268   \n",
      "436869 -0.563763  ...                       -0.384075              -0.385262   \n",
      "437071  0.398827  ...                        0.108396              -0.957296   \n",
      "437041  1.017891  ...                        0.238398              -0.994832   \n",
      "\n",
      "        Soil_Smectite_30m_max  Soil_Smectite_30m_mean  Soil_Illite_30m_min  \\\n",
      "457                 -0.509533               -0.257755             0.482723   \n",
      "1239                -0.339019                0.358514             2.275967   \n",
      "816                 -0.525760               -0.560859            -0.845022   \n",
      "862                 -0.338268                0.125884            -0.823290   \n",
      "441                 -0.882135               -0.758649             0.171613   \n",
      "...                       ...                     ...                  ...   \n",
      "436788               1.137795                0.634677            -1.829547   \n",
      "435862               1.041795                1.772744             0.325342   \n",
      "436869              -0.769220               -0.772334            -0.133939   \n",
      "437071               0.478744               -0.854450             0.889254   \n",
      "437041               0.782533               -0.478977             0.843070   \n",
      "\n",
      "        Soil_Illite_30m_max  Soil_Illite_30m_mean      NDVI  mean_Red  \\\n",
      "457                1.057385              0.927547 -0.400507 -0.102074   \n",
      "1239               0.780383              1.638911 -0.397354  0.207173   \n",
      "816               -1.099953             -0.962602  0.310958 -0.311759   \n",
      "862               -1.410754             -1.666711 -0.097194 -0.101076   \n",
      "441                0.056473             -0.284982  0.122080 -0.716788   \n",
      "...                     ...                   ...       ...       ...   \n",
      "436788             0.453648              0.008449 -0.096962 -0.369280   \n",
      "435862             1.671728              0.605813 -0.202112  1.470794   \n",
      "436869            -0.578453             -0.481532 -0.177622 -0.674359   \n",
      "437071             0.201187              0.498908 -1.334009  3.875402   \n",
      "437041             0.803605              0.982388 -1.348236  1.807485   \n",
      "\n",
      "        mean_NIR  \n",
      "457    -0.807633  \n",
      "1239   -0.352932  \n",
      "816     0.517699  \n",
      "862    -0.212228  \n",
      "441    -0.853391  \n",
      "...          ...  \n",
      "436788 -0.663489  \n",
      "435862  2.070305  \n",
      "436869 -1.331489  \n",
      "437071  1.253846  \n",
      "437041 -0.357983  \n",
      "\n",
      "[21764 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 13\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import dask_geopandas as dgpd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def main():\n",
    "    # Setup Dask Client\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data as Dask GeoDataFrame\n",
    "    file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Define target and features\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [target_column, 'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "    \n",
    "    # Fill NaN values\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Scale features and convert to Dask DataFrame\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled_np = scaler.fit_transform(features).compute()  # Compute to NumPy array\n",
    "    features_scaled_df = pd.DataFrame(features_scaled_np, columns=features.columns)  # Convert to Pandas DataFrame\n",
    "    features_scaled_dask_df = dd.from_pandas(features_scaled_df, npartitions=16)  # Convert to Dask DataFrame\n",
    "    \n",
    "    # Sample 5% of the rows randomly\n",
    "    sample = features_scaled_dask_df.sample(frac=0.05, random_state=42)\n",
    "    target_sample = target.sample(frac=0.05, random_state=42)\n",
    "    \n",
    "    # Compute the samples\n",
    "    sample = sample.compute()\n",
    "    target_sample = target_sample.compute()\n",
    "    \n",
    "    print(sample)\n",
    "\n",
    "    # Preliminary SHAP analysis\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(sample, target_sample)\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sample)\n",
    "\n",
    "    # Summary plot for SHAP values\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, sample, feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"SHAP analysis completed and plots saved.\")\n",
    "    \n",
    "    # Select top 30% features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]\n",
    "    selected_features = features.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Train-test split with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train model using RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "    # Evaluate and plot results\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "    print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "# # Import necessary libraries\n",
    "# import dask_geopandas as dgpd\n",
    "# from dask.distributed import Client\n",
    "# from dask_ml.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from dask_ml.model_selection import train_test_split\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# def main():\n",
    "#     # Setup Dask Client\n",
    "#     client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "#     print(client)\n",
    "\n",
    "#     # Load data as Dask GeoDataFrame\n",
    "#     file_path = \"C://Users//M.Ibrah//OneDrive - Department of Primary Industries And Regional Development//Desktop//Food Agility project-homework//WA-Rainfall-Zone-Analysis//Complete dataset//updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "#     data = dgpd.read_file(file_path, npartitions=256)\n",
    "#     print(data.shape)\n",
    "\n",
    "#     # Define target and features\n",
    "#     target_column = 'yield'\n",
    "#     drop_columns = [target_column, 'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', 'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry']\n",
    "#     features = data.drop(columns=drop_columns)\n",
    "#     target = data[target_column]\n",
    "    \n",
    "#     # Fill NaN values\n",
    "#     features = features.fillna(0)\n",
    "    \n",
    "#     # Scale features and convert to Dask DataFrame\n",
    "#     scaler = StandardScaler()\n",
    "#     features_scaled_np = scaler.fit_transform(features).compute()  # Compute to NumPy array\n",
    "#     features_scaled_df = pd.DataFrame(features_scaled_np, columns=features.columns)  # Convert to Pandas DataFrame\n",
    "#     features_scaled_dask_df = dd.from_pandas(features_scaled_df, npartitions=16)  # Convert to Dask DataFrame\n",
    "    \n",
    "#     # Combine features and target into one DataFrame\n",
    "#     combined_dask_df = dd.concat([features_scaled_dask_df, target], axis=1)\n",
    "#     combined_dask_df.columns = list(features.columns) + [target_column]\n",
    "\n",
    "#     # Sample 5% of the rows randomly from combined DataFrame\n",
    "#     sampled_combined_df = combined_dask_df.sample(frac=0.05, random_state=42).compute()\n",
    "\n",
    "#     # Separate features and target after sampling\n",
    "#     sample = sampled_combined_df.drop(columns=[target_column])\n",
    "#     target_sample = sampled_combined_df[target_column]\n",
    "    \n",
    "#     print(sample)\n",
    "\n",
    "#     # Preliminary SHAP analysis\n",
    "#     preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "#     with ProgressBar():\n",
    "#         preliminary_rf_model.fit(sample, target_sample)\n",
    "\n",
    "#     explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "#     shap_values = explainer.shap_values(sample)\n",
    "\n",
    "#     # Summary plot for SHAP values\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     shap.summary_plot(shap_values, sample, feature_names=features.columns, show=False)\n",
    "#     plt.savefig(\"shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "#     print(\"SHAP analysis completed and plots saved.\")\n",
    "    \n",
    "#     # Select top 30% features based on SHAP importance\n",
    "#     num_top_features = int(0.3 * features.shape[1])\n",
    "#     top_feature_indices = np.argsort(np.abs(shap_values)).mean(axis=0)[-num_top_features:]\n",
    "#     selected_features = features.iloc[:, top_feature_indices]\n",
    "#     selected_feature_names = features.columns[top_feature_indices]\n",
    "#     print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "#     # Train-test split with selected features\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "#     # Train model using RandomForestRegressor\n",
    "#     rf_model = RandomForestRegressor(n_estimators=150, n_jobs=16)\n",
    "#     with ProgressBar():\n",
    "#         rf_model.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "#     # Evaluate and plot results\n",
    "#     y_pred = rf_model.predict(X_test.compute())\n",
    "#     mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "#     r2 = r2_score(y_test.compute(), y_pred)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "#     plt.xlabel(\"Actual Yield\")\n",
    "#     plt.ylabel(\"Predicted Yield\")\n",
    "#     plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "#     plt.savefig(\"prediction_plot_rf.png\", bbox_inches='tight')\n",
    "    \n",
    "#     print(\"Distributed Random Forest processing complete.\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c94b25",
   "metadata": {},
   "source": [
    "# Cell 14: This cell imports all the required Python packages. Ensure packages such as `pandas`, `numpy`, `sklearn`, and `shap` are installed. Use `pip install package_name` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# Description: Import required libraries for data manipulation, machine learning, and SHAP.\n",
    "# inputs = \"updated_boundaries_with_NDVI_for_2022.gpkg\", just library imports.\n",
    "# Requirements: Install pandas, numpy, sklearn, shap, matplotlib. Optionally geopandas if used.\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask_geopandas as dgpd\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    # Initialize Dask Client\n",
    "    client = Client(n_workers=16, threads_per_worker=2, memory_limit='8GB', dashboard_address=':8786')\n",
    "    print(client)\n",
    "\n",
    "    # Load data directly as Dask GeoDataFrame for optimized distributed processing\n",
    "    file_path = \"/scratch/pawsey0988/mibrahim/updated_boundaries_with_NDVI_for_2022.gpkg\"\n",
    "    data = dgpd.read_file(file_path, npartitions=256)\n",
    "    print(\"Data Shape:\", data.shape)\n",
    "\n",
    "    # Define target and feature columns\n",
    "    target_column = 'yield'\n",
    "    drop_columns = [\n",
    "        target_column, 'ID', 'OBJECTID', 'year', 'crop_name', 'pred_prob', 'area_ha', \n",
    "        'lga_name', 'production', 'Shape__Length', 'Shape__Area', 'geometry'\n",
    "    ]\n",
    "    features = data.drop(columns=drop_columns)\n",
    "    target = data[target_column]\n",
    "\n",
    "    # Fill NaN values in features to avoid missing data issues\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    # Scale features using Dask StandardScaler without converting to a dense array\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled_dask = scaler.fit_transform(features)  # Stay as Dask DataFrame\n",
    "\n",
    "    # Sample a subset (5%) for preliminary SHAP analysis\n",
    "    sampled_combined_df = features_scaled_dask.sample(frac=0.05, random_state=42)\n",
    "    sampled_target = target.sample(frac=0.05, random_state=42)\n",
    "\n",
    "    # Preliminary SHAP analysis on the sampled data\n",
    "    print(\"Starting preliminary SHAP analysis...\")\n",
    "    preliminary_rf_model = RandomForestRegressor(n_estimators=150, n_jobs=-1)\n",
    "    with ProgressBar():\n",
    "        preliminary_rf_model.fit(sampled_combined_df.compute(), sampled_target.compute())\n",
    "\n",
    "    explainer = shap.TreeExplainer(preliminary_rf_model)\n",
    "    shap_values = explainer.shap_values(sampled_combined_df.compute())\n",
    "\n",
    "    # Plot SHAP summary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, sampled_combined_df.compute(), feature_names=features.columns, show=False)\n",
    "    plt.savefig(\"/scratch/pawsey0988/mibrahim/shap_summary_plot_before_training_rf.png\", bbox_inches='tight')\n",
    "    print(\"SHAP summary plot saved.\")\n",
    "\n",
    "    # Select top 30% of features based on SHAP importance\n",
    "    num_top_features = int(0.3 * features.shape[1])\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values).mean(axis=0))[-num_top_features:]\n",
    "    selected_features = features_scaled_dask.iloc[:, top_feature_indices]\n",
    "    selected_feature_names = features.columns[top_feature_indices]\n",
    "    print(f\"Selected top 30% features: {selected_feature_names}\")\n",
    "\n",
    "    # Split data into train-test sets using the selected features\n",
    "    print(\"Starting train-test split...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Train the final RandomForest model using selected features\n",
    "    print(\"Training RandomForestRegressor on selected features...\")\n",
    "    rf_model = RandomForestRegressor(n_estimators=150, n_jobs=-1)\n",
    "    with ProgressBar():\n",
    "        rf_model.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred = rf_model.predict(X_test.compute())\n",
    "    mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "    r2 = r2_score(y_test.compute(), y_pred)\n",
    "    print(f\"Model evaluation - MSE: {mse:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "    # Scatter plot of actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.compute(), y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Yield\")\n",
    "    plt.ylabel(\"Predicted Yield\")\n",
    "    plt.title(f\"Random Forest Prediction (R2={r2:.2f}, MSE={mse:.2f})\")\n",
    "    plt.savefig(\"/scratch/pawsey0988/mibrahim/prediction_plot_rfnew.png\", bbox_inches='tight')\n",
    "    print(\"Prediction plot saved. Model training and evaluation complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
